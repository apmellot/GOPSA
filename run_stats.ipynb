{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "251a30ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import scipy.stats\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []\n",
    "# get all filnames beginning with results/geodesic_optimization_results_\n",
    "filenames = os.listdir('./results/')\n",
    "print(len(filenames))\n",
    "filenames = [f for f in filenames if re.match(\n",
    "    r'geodesic_optimization_results_\\d+', f\n",
    "    )]\n",
    "filenames = sorted(filenames, key=lambda x: int(re.search(r'\\d+', x).group()))\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(f'./results/{filename}', index_col=0)\n",
    "    df_all.append(df)\n",
    "df_all = pd.concat(df_all)\n",
    "df_all.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1b64055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def corrected_std(differences, n_train, n_test):\n",
    "    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : ndarray of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrected_std : float\n",
    "        Variance-corrected standard deviation of the set of differences.\n",
    "    \"\"\"\n",
    "    # kr = k times r, r times repeated k-fold crossvalidation,\n",
    "    # kr equals the number of times the model was evaluated\n",
    "    kr = len(differences)\n",
    "    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\n",
    "    corrected_std = np.sqrt(corrected_var)\n",
    "    return corrected_std\n",
    "\n",
    "\n",
    "def compute_corrected_ttest(differences, df, n_train, n_test):\n",
    "    \"\"\"Computes right-tailed paired t-test with corrected variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : array-like of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    df : int\n",
    "        Degrees of freedom.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Variance-corrected t-statistic.\n",
    "    p_val : float\n",
    "        Variance-corrected p-value.\n",
    "    \"\"\"\n",
    "    mean = np.mean(differences)\n",
    "    std = corrected_std(differences, n_train, n_test)\n",
    "    t_stat = mean / std\n",
    "    return t_stat\n",
    "\n",
    "\n",
    "def get_p_val(t_stat, df=100):\n",
    "    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n",
    "    return p_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64de5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sizes = {1: 739, 2: 389, 3: 294, 4: 592, 5: 1093}\n",
    "contrast_pairs = [\n",
    "    ('dummy', 'baseline_no_recenter'),\n",
    "    ('dummy', 'baseline_recenter'),\n",
    "    ('baseline_green', 'baseline_fit_intercept'),\n",
    "    ('baseline_green', 'geodesic_optim'),\n",
    "    ('baseline_no_recenter', 'baseline_fit_intercept'),\n",
    "    ('baseline_fit_intercept', 'geodesic_optim')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2170c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_vals = list()\n",
    "for ref, pro in contrast_pairs:\n",
    "    for source in df_all.sites_source_index.unique():\n",
    "    # for source in [1]:\n",
    "        source = int(source)\n",
    "    #     print(source, target)\n",
    "        df_ref = df_all.query(\n",
    "            f\"method == '{ref}' & sites_source_index == {source}\"\n",
    "        ).reset_index(drop=True)\n",
    "        df_pro = df_all.query(\n",
    "            f\"method == '{pro}' & sites_source_index == {source}\"\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        assert np.all(df_ref.index == df_pro.index)\n",
    "        assert np.all(df_ref.y_true == df_pro.y_true)\n",
    "        differences = list()\n",
    "        for gg, index in df_ref.groupby('split_index').groups.items():\n",
    "            df_sub1 = df_ref.iloc[index]\n",
    "            df_sub2 = df_pro.iloc[index]\n",
    "            sp_ref = scipy.stats.spearmanr(df_sub1['y_true'], df_sub1['y_pred']).statistic\n",
    "            sp_pro = scipy.stats.spearmanr(df_sub2['y_true'], df_sub2['y_pred']).statistic\n",
    "            mae_ref = sklearn.metrics.mean_absolute_error(df_sub1['y_true'], df_sub1['y_pred'])\n",
    "            mae_pro = sklearn.metrics.mean_absolute_error(df_sub2['y_true'], df_sub2['y_pred'])\n",
    "            r2_ref = sklearn.metrics.r2_score(df_sub1['y_true'], df_sub1['y_pred'])\n",
    "            r2_pro = sklearn.metrics.r2_score(df_sub2['y_true'], df_sub2['y_pred'])\n",
    "\n",
    "            differences.append(\n",
    "                dict(spearman=sp_pro - sp_ref,\n",
    "                     mae=mae_pro - mae_ref,\n",
    "                     r2=r2_pro - r2_ref)\n",
    "            )\n",
    "        differences = pd.DataFrame(differences)\n",
    "        n_train = source_sizes[source]\n",
    "        n_test = int(len(df_sub1['y_pred']) // 2)\n",
    "        df_p = differences.apply(lambda x: get_p_val(compute_corrected_ttest(differences=x, df=100, n_train=n_train, n_test=n_test)))\n",
    "        df_p['contr'] = f'{pro}-{ref}'\n",
    "        df_p['source'] = source\n",
    "        p_vals.append(df_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c6f0176",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pval = pd.concat(p_vals, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66a56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1a81c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pval.to_csv('./results/p_values.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synapse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
